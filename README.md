# ðŸ“° News Aggregator - Multi-Scraper con Scrapy

Un proyecto de web scraping que agrega noticias de tres fuentes diferentes (**La Voz de Galicia**, **El PaÃ­s** y **Marca**) en un formato unificado utilizando Scrapy.

##  Objetivo del Proyecto

Demostrar el uso de diferentes tÃ©cnicas de scraping (selectores CSS, XPath y mixtos) para extraer informaciÃ³n de mÃºltiples sitios web de noticias y consolidarla en un Ãºnico archivo JSON.

##  CaracterÃ­sticas

- **Tres spiders independientes**:
  - **La Voz de Galicia Spider**: Utiliza Ãºnicamente selectores CSS
  - **El PaÃ­s Spider**: Utiliza Ãºnicamente selectores XPath
  - **Marca Spider**: Combina selectores CSS y XPath

- **ExtracciÃ³n de datos (5 campos comunes)**:
  - Fuente (identificador del sitio web)
  - TÃ­tulo de la noticia
  - Fecha de publicaciÃ³n
  - URL del artÃ­culo
  - Body (texto del artÃ­culo)

- **Limpieza de datos**:
  - EliminaciÃ³n de espacios en blanco innecesarios
  - EliminaciÃ³n de saltos de lÃ­nea (`\n`, `\r`)
  - NormalizaciÃ³n de nombres de autores
  - Manejo de datos faltantes con valores por defecto

##  Requisitos

```bash
Python 3.8+
Scrapy 2.11+
```

## ðŸ”§ InstalaciÃ³n

1. **Clonar el repositorio**:
```bash
git clone https://github.com/EnriqueJimenezMartinez/Aggregator_Scraper.git
cd Aggregator_Scraper
```

2. **Crear un entorno virtual (recomendado)**:
```bash
python -m venv venv
source venv/bin/activate  # En Linux/Mac
# o
venv\Scripts\activate  # En Windows
```

3. **Instalar dependencias**:
```bash
pip install scrapy
```

##  Uso

### Ejecutar cada spider individualmente

Para generar el archivo `news_data.json` con todas las noticias de las tres fuentes:

```bash
# Navega al directorio del proyecto
cd news_aggregator

# 1. Ejecutar La Voz de Galicia (solo CSS)
scrapy crawl lavoz -o ../temp_lavoz.json

# 2. Ejecutar El PaÃ­s (solo XPath)
scrapy crawl elpais -o ../temp_elpais.json

# 3. Ejecutar Marca (mixto)
scrapy crawl marca -o ../temp_marca.json

# 4. Volver al directorio raÃ­z y combinar los resultados
cd ..
python3 -c "
import json
lavoz = json.load(open('temp_lavoz.json', encoding='utf-8'))
elpais = json.load(open('temp_elpais.json', encoding='utf-8'))
marca = json.load(open('temp_marca.json', encoding='utf-8'))
all_news = lavoz + elpais + marca
json.dump(all_news, open('news_data.json', 'w', encoding='utf-8'), ensure_ascii=False, indent=2)
"

# 5. Limpiar archivos temporales
rm temp_*.json
```

##  Estructura del Proyecto

```
Aggregator_Scraper/
â”‚
â”œâ”€â”€ news_aggregator/              # Proyecto Scrapy
â”‚   â”œâ”€â”€ __init__.py               # Inicializador del mÃ³dulo
â”‚   â”œâ”€â”€ items.py                  # DefiniciÃ³n de items 
â”‚   â”œâ”€â”€ middlewares.py            # Middlewares personalizados
â”‚   â”œâ”€â”€ pipelines.py              # Pipelines de procesamiento
â”‚   â”œâ”€â”€ settings.py               # ConfiguraciÃ³n del proyecto
â”‚   â””â”€â”€ spiders/                  # Directorio de spiders
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ lavoz.py              # Spider La Voz (CSS)
â”‚       â”œâ”€â”€ elpais.py             # Spider El PaÃ­s (XPath)
â”‚       â””â”€â”€ marca.py              # Spider Marca (Mixto)
â”‚
â”œâ”€â”€ news_data.json                # Archivo de salida con los datos
â”œâ”€â”€ .gitignore                    # Archivos ignorados por Git
â””â”€â”€ README.md                     # Este archivo
```

##  Formato de Salida

Los datos se exportan en formato JSON con la siguiente estructura:

```json
{
  "source": "La Voz de Galicia",
  "title": "TÃ­tulo de la noticia",
  "date": "2026-01-30",
  "url": "https://www.lavozdegalicia.es/noticia/galicia/2026/01/30/ejemplo.html",
  "body": "Texto del artÃ­culo..."
}
```

##  Detalles TÃ©cnicos

### La Voz de Galicia Spider (CSS Selectors)
- **TÃ©cnica**: Selectores CSS exclusivamente
- **Datos extraÃ­dos**: TÃ­tulo, fecha, URL, body
- **Particularidades**: El tÃ­tulo se obtiene del `<title>` del artÃ­culo

### El PaÃ­s Spider (XPath Selectors)
- **TÃ©cnica**: XPath exclusivamente
- **Datos extraÃ­dos**: TÃ­tulo, fecha, URL, body

### Marca Spider (Selectores Mixtos)
- **TÃ©cnica**: CombinaciÃ³n de CSS y XPath
- **Datos extraÃ­dos**: TÃ­tulo, fecha, URL, body
- **Particularidades**: Scraping de la portada principal de Marca

##  ConfiguraciÃ³n

El archivo `settings.py` incluye configuraciÃ³n responsable:

- **DOWNLOAD_DELAY**: 2 segundos entre peticiones
- **CONCURRENT_REQUESTS_PER_DOMAIN**: 1 peticiÃ³n por dominio a la vez



##  Licencia

Este proyecto es de cÃ³digo abierto y estÃ¡ disponible para fines educativos.

